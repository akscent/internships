{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":29296,"databundleVersionId":2344227,"sourceType":"competition"},{"sourceId":7685567,"sourceType":"datasetVersion","datasetId":4484528,"isSourceIdPinned":true}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !python3 -m venv myenv\n# !source myenv/bin/activate\n!pip uninstall pandas -y\n!pip install --upgrade pip > installations.txt\n!pip install pandas==1.4.3 pyarrow yellowbrick polars transformers nltk gensim lightautoml > installations.txt\n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T05:01:29.274767Z","iopub.execute_input":"2024-02-26T05:01:29.275388Z","iopub.status.idle":"2024-02-26T05:05:27.335406Z","shell.execute_reply.started":"2024-02-26T05:01:29.275334Z","shell.execute_reply":"2024-02-26T05:05:27.332807Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Found existing installation: pandas 2.2.0\nUninstalling pandas-2.2.0:\n  Successfully uninstalled pandas-2.2.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.13 requires pandas, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.0.11 which is incompatible.\nfeaturetools 1.28.0 requires pandas>=1.5.0, but you have pandas 1.4.3 which is incompatible.\nfitter 1.7.0 requires joblib<2.0.0,>=1.3.1, but you have joblib 1.2.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.8.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nplotnine 0.12.4 requires pandas>=1.5.0, but you have pandas 1.4.3 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.4.3 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflowjs 4.16.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\ntorchaudio 2.1.2+cpu requires torch==2.1.2, but you have torch 2.0.0 which is incompatible.\ntorchtext 0.16.2+cpu requires torch==2.1.2, but you have torch 2.0.0 which is incompatible.\ntorchvision 0.16.2+cpu requires torch==2.1.2, but you have torch 2.0.0 which is incompatible.\nxarray 2024.1.0 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nxarray 2024.1.0 requires pandas>=1.5, but you have pandas 1.4.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip uninstall torch -y > installations.txt\n!pip install torch==2.1.0 > installations.txt","metadata":{"execution":{"iopub.status.busy":"2024-02-26T05:18:24.415629Z","iopub.execute_input":"2024-02-26T05:18:24.416095Z","iopub.status.idle":"2024-02-26T05:20:49.031468Z","shell.execute_reply.started":"2024-02-26T05:18:24.416063Z","shell.execute_reply":"2024-02-26T05:20:49.030013Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlightautoml 0.3.8.1 requires torch<=2.0.0,>=1.9.0, but you have torch 2.1.0 which is incompatible.\ntorchaudio 2.1.2+cpu requires torch==2.1.2, but you have torch 2.1.0 which is incompatible.\ntorchtext 0.16.2+cpu requires torch==2.1.2, but you have torch 2.1.0 which is incompatible.\ntorchvision 0.16.2+cpu requires torch==2.1.2, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import polars as pl\nimport numpy as np\nimport pyarrow as pa\nimport os\nimport time\nimport optuna\nimport requests\nimport sys\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport torch\nfrom copy import deepcopy as copy\nimport torch.nn as nn\nfrom collections import OrderedDict\nfrom collections import Counter\n\nfrom lightautoml.automl.base import AutoML\nfrom lightautoml.ml_algo.boost_lgbm import BoostLGBM\nfrom lightautoml.ml_algo.tuning.optuna import OptunaTuner\nfrom lightautoml.pipelines.features.lgb_pipeline import LGBSimpleFeatures\nfrom lightautoml.pipelines.ml.base import MLPipeline\nfrom lightautoml.pipelines.selection.importance_based import ImportanceCutoffSelector, ModelBasedImportanceEstimator\nfrom lightautoml.reader.base import PandasToPandasReader\nfrom lightautoml.tasks import Task\nfrom lightautoml.automl.blend import WeightedBlender\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML\nfrom lightautoml.tasks import Task\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-26T05:20:49.034093Z","iopub.execute_input":"2024-02-26T05:20:49.034507Z","iopub.status.idle":"2024-02-26T05:21:36.633037Z","shell.execute_reply.started":"2024-02-26T05:20:49.034471Z","shell.execute_reply":"2024-02-26T05:21:36.631719Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/input/sbermarket-internship-competition/sample_submission.csv\n/kaggle/input/sbermarket-internship-competition/train.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"print(pd.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T05:21:36.635297Z","iopub.execute_input":"2024-02-26T05:21:36.636272Z","iopub.status.idle":"2024-02-26T05:21:36.642790Z","shell.execute_reply.started":"2024-02-26T05:21:36.636228Z","shell.execute_reply":"2024-02-26T05:21:36.641066Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"1.4.3\n","output_type":"stream"}]},{"cell_type":"code","source":"def ohe_data(raw: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Вовращает ohe матрицу для категорий.\n\n    Args:\n        raw (pl.DataFrame): Polars DataFrame.\n\n    Returns:\n        (tpl.DataFrame).\n    \"\"\"\n\n    # OHE 'cart', группировка по юзеру и дате заказа по максимальным значениям\n    # Таким образом для каждой категории, если она была в этот день в заказе у пользователя, будет 1\n    train_raw = raw.to_dummies(columns='cart').group_by(['user_id', 'order_completed_at']).max()\n    train_raw = train_raw.sort(['user_id', 'order_completed_at'])\n\n    return train_raw\n    \ndef sep_history(train_raw: pl.DataFrame) -> tuple[pl.DataFrame, pl.DataFrame]:\n    \"\"\"\n    Обрабатывает датафрейм polars и возвращает обработынный тренировочный и валидационный датасеты.\n\n    Args:\n        raw (pl.DataFrame): Polars DataFrame для обработки.\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame]: Обработанные DataFrame для train_data и valid_data.\n    \"\"\"\n    \n    # Вычисление переменных времени\n    train_raw = train_raw.with_columns(pl.col(\"order_completed_at\").str.to_datetime())\n    train_raw = train_raw.with_columns(\n        [\n            (pl.col('user_id').cum_count() - pl.lit(1)).over(['user_id']).alias('order_number'),\n            pl.col(\"order_completed_at\").dt.hour().alias(\"hour\"),\n            pl.col(\"order_completed_at\").dt.week().alias(\"week\"),\n            pl.col(\"order_completed_at\").dt.weekday().alias(\"weekday\"),\n            pl.col(\"order_completed_at\").dt.day().alias(\"day\"),\n            pl.col(\"order_completed_at\").dt.month().alias(\"month\"),\n            pl.col(\"order_completed_at\").dt.year().alias(\"year\"),\n        ]\n    )\n    train_raw = train_raw.drop('order_completed_at')\n    \n    # Сепарация последнего заказа из истории заказов\n    grouped = train_raw.group_by('user_id').agg(pl.max('order_number').alias('max_order_number'))\n    merged = train_raw.join(grouped, on='user_id')\n    last_order = merged['max_order_number'] == merged['order_number']\n\n    # Разделение на train_data (заказы, кроме последнего) и valid_data (последний заказ) .group_by('user_id').sum()\n    train_data = train_raw.filter(~last_order)\n    valid_data = train_raw.filter(last_order).drop('hour', 'week', 'weekday', 'day', 'month', 'year')\n\n    return train_data, valid_data\n\ndef create_dataset(train_data: pl.DataFrame, valid_data: pl.DataFrame)-> pd.DataFrame:\n    \"\"\"\n    Компилирует историю заказов (train_data) и последний заказ (valid_data) в укомплектованный тренировочный pd.Dataframe \"Train\".\n\n    Args:\n        train_data (pl.DataFrame): история заказов (корзина на каждого юзера).\n        valid_data (pl.DataFrame): последний заказ (корзина на каждого юзера).\n\n    Returns:\n        pd.DataFrame: X + Y for model.\n    \"\"\"\n    # Преобразование в длинный формат юзера и категории товара\n    train_melt = train_data.drop('order_number').melt(id_vars=['user_id', 'hour', 'week', 'weekday', 'day', 'month', 'year'], variable_name='category', value_name='ordered')\n    valid_melt = val_data.drop('order_number',).melt(id_vars=['user_id', ], variable_name='category', value_name='target')\n\n    # Тренировочный датасет\n    Train = train_melt\n    order_number_df = val_data.select(['user_id', 'order_number']).unique() # Количество заказов у юзера\n    dl_tmp = Train.group_by('category').agg(pl.col(['ordered']).sum()) # Количество заказов по категории\n    Train = Train.join(order_number_df, on='user_id').rename({\"order_number\": \"total_order_num\"})\n    Train = Train.join(dl_tmp, on='category').rename({\"ordered_right\": \"total_order_in_cat\"})\n    # Вычисление рейтинга для каждой записи\n    Train = Train.with_columns(\n        [\n            (pl.col('ordered') / pl.col('total_order_num')).alias('total_rating'),\n            (pl.col('user_id').cast(pl.Utf8) + ';' + pl.col('category')).alias('id')\n        ]\n    )\n    Train = Train.group_by('user_id', 'category', 'id').agg(\n        pl.col(['hour']).mean().name.suffix(\"_mean\"), pl.col(['hour']).max().name.suffix(\"_max\"),\n        pl.col(['hour']).min().name.suffix(\"_min\"), pl.col(['hour']).std().name.suffix(\"_std\"),\n        pl.col(['week']).mean().name.suffix(\"_mean\"), pl.col(['week']).max().name.suffix(\"_max\"),\n        pl.col(['week']).min().name.suffix(\"_min\"), pl.col(['week']).std().name.suffix(\"_std\"),\n        pl.col(['weekday']).mean().name.suffix(\"_mean\"), pl.col(['weekday']).max().name.suffix(\"_max\"),\n        pl.col(['weekday']).min().name.suffix(\"_min\"), pl.col(['weekday']).std().name.suffix(\"_std\"),\n        pl.col(['day']).mean().name.suffix(\"_mean\"), pl.col(['day']).max().name.suffix(\"_max\"),\n        pl.col(['day']).min().name.suffix(\"_min\"), pl.col(['day']).std().name.suffix(\"_std\"),\n        pl.col(['month']).mean().name.suffix(\"_mean\"), pl.col(['month']).max().name.suffix(\"_max\"),\n        pl.col(['month']).min().name.suffix(\"_min\"), pl.col(['month']).std().name.suffix(\"_std\"),\n        pl.col(['year']).mean().name.suffix(\"_mean\"), pl.col(['year']).max().name.suffix(\"_max\"),\n        pl.col(['year']).min().name.suffix(\"_min\"), pl.col(['year']).std().name.suffix(\"_std\"),\n        pl.col(['total_rating']).mean().name.suffix(\"_mean\"), pl.col(['total_rating']).max().name.suffix(\"_max\"),\n        pl.col(['total_rating']).min().name.suffix(\"_min\"), pl.col(['total_rating']).std().name.suffix(\"_std\"),\n        pl.col(['ordered']).mean().name.suffix(\"_mean\"), pl.col(['ordered']).max().name.suffix(\"_max\"),\n        pl.col(['ordered']).min().name.suffix(\"_min\"), pl.col(['ordered']).std().name.suffix(\"_std\"),\n        pl.col(['total_order_in_cat']).mean().name.suffix(\"_mean\"), pl.col(['total_order_in_cat']).max().name.suffix(\"_max\"),\n        pl.col(['total_order_in_cat']).min().name.suffix(\"_min\"), pl.col(['total_order_in_cat']).std().name.suffix(\"_std\"),\n        pl.col(['total_order_num']).mean().name.suffix(\"_mean\"), pl.col(['total_order_num']).max().name.suffix(\"_max\"),\n        pl.col(['total_order_num']).min().name.suffix(\"_min\"), pl.col(['total_order_num']).std().name.suffix(\"_std\"),\n    )\n    # Вычисление рейтинга по времени\n    rating_per_hour = Train.group_by(['year_mean', 'month_mean', 'hour_mean', 'user_id']).agg(\n        ((pl.col(\"total_rating_mean\").mean()) * 100).alias(\"rating_per_hour_mean\"),\n        ((pl.col(\"total_rating_mean\").std()) * 100).alias(\"rating_per_hour_std\"),\n        ((pl.col(\"total_rating_mean\").sum())).alias(\"rating_per_hour_sum\"),\n        ((pl.col(\"total_rating_mean\").median()) * 100).alias(\"rating_per_hour_median\"),\n    )\n\n    rating_per_weekday = Train.group_by(['year_mean', 'month_mean', 'weekday_mean', 'user_id']).agg(\n        ((pl.col(\"total_rating_mean\").mean()) * 100).alias(\"rating_per_w_mean\"),\n        ((pl.col(\"total_rating_mean\").std()) * 100).alias(\"rating_per_w_std\"),\n        ((pl.col(\"total_rating_mean\").sum())).alias(\"rating_per_w_sum\"),\n        ((pl.col(\"total_rating_mean\").median()) * 100).alias(\"rating_per_w_median\"),\n    )\n    rating_per_day = Train.group_by(['year_mean', 'month_mean', 'day_mean', 'user_id']).agg(\n        ((pl.col(\"total_rating_mean\").mean()) * 100).alias(\"rating_per_d_mean\"),\n        ((pl.col(\"total_rating_mean\").std()) * 100).alias(\"rating_per_d_std\"),\n        ((pl.col(\"total_rating_mean\").sum())).alias(\"rating_per_d_sum\"),\n        ((pl.col(\"total_rating_mean\").median()) * 100).alias(\"rating_per_d_median\"),\n    )\n\n    rating_per_month = Train.group_by(['year_mean', 'month_mean', 'user_id']).agg(\n        ((pl.col(\"total_rating_mean\").mean()) * 100).alias(\"rating_per_m_mean\"),\n        ((pl.col(\"total_rating_mean\").std()) * 100).alias(\"rating_per_m_std\"),\n        ((pl.col(\"total_rating_mean\").sum())).alias(\"rating_per_m_sum\"),\n        ((pl.col(\"total_rating_mean\").median()) * 100).alias(\"rating_per_m_median\"),\n    )\n\n\n    Train = Train.join(rating_per_month, on=['year_mean', 'month_mean', 'user_id'])\n    Train = Train.join(rating_per_weekday, on=['year_mean', 'month_mean', 'weekday_mean', 'user_id'])\n    Train = Train.join(rating_per_day, on=['year_mean', 'month_mean', 'day_mean', 'user_id'])\n    Train = Train.join(rating_per_hour, on=['year_mean', 'month_mean', 'hour_mean', 'user_id'])\n    \n    # Присоединение целевой переменной из valid_melt к Train\n    Train = Train.join(valid_melt, on = ['user_id', 'category'])\n#     Train = Train.drop('user_id', 'category')\n\n    # Преобразование в pandas DataFrame\n    Train = Train.to_pandas()\n    Train['id'] = Train['id'].str.replace('cart_', '')\n    Train['category'] = Train['category'].str.replace('cart_', '')\n\n    return Train","metadata":{"execution":{"iopub.status.busy":"2024-02-26T05:21:36.645634Z","iopub.execute_input":"2024-02-26T05:21:36.646239Z","iopub.status.idle":"2024-02-26T05:21:36.697021Z","shell.execute_reply.started":"2024-02-26T05:21:36.646207Z","shell.execute_reply":"2024-02-26T05:21:36.695810Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tmp импорт в pandas\n\nraw = pd.read_csv('../input/sbermarket-internship-competition/train.csv')\nsub = pd.read_csv('../input/sbermarket-internship-competition/sample_submission.csv', sep = \",\")\n\n# Приведение столбца 'cart' к int\nraw['cart'] = raw['cart'].astype(int)\n\ndef filter_raw_data(raw, sub):\n    users = map(int, (x.split(';')[0] for x in sub['id']))\n    user_counts = Counter(users)\n    frequent_users = {user for user, count in user_counts.items() if count > 25}\n    filtered_raw = raw[raw['user_id'].isin(frequent_users)]\n    filtered_sub = sub[sub['id'].apply(lambda x: int(x.split(';')[0])).isin(frequent_users)]\n    total_count = sum(user_counts.values())\n    frequent_count = sum(count for user, count in user_counts.items() if user in frequent_users)\n    proportion = frequent_count / total_count\n    \n    return filtered_raw, filtered_sub, proportion\n\nfiltered_raw, filtered_sub, proportion = filter_raw_data(raw, sub)\nprint(f\"Процент наблюдений, используемый для тренировки: {proportion:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-02-26T05:21:36.698981Z","iopub.execute_input":"2024-02-26T05:21:36.699832Z","iopub.status.idle":"2024-02-26T05:21:41.800124Z","shell.execute_reply.started":"2024-02-26T05:21:36.699788Z","shell.execute_reply":"2024-02-26T05:21:41.798739Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Процент наблюдений, используемый для тренировки: 0.97%\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nfiltered_raw = pl.from_pandas(filtered_raw)\ntrain_raw = ohe_data(filtered_raw)\ntrain_data, val_data = sep_history(train_raw)\nTrain = create_dataset(train_data, val_data)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T05:21:41.801588Z","iopub.execute_input":"2024-02-26T05:21:41.802004Z","iopub.status.idle":"2024-02-26T05:23:32.493291Z","shell.execute_reply.started":"2024-02-26T05:21:41.801972Z","shell.execute_reply":"2024-02-26T05:23:32.492327Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"CPU times: user 3min 9s, sys: 1min 9s, total: 4min 19s\nWall time: 1min 50s\n","output_type":"stream"}]},{"cell_type":"code","source":"Train","metadata":{"execution":{"iopub.status.busy":"2024-02-26T05:23:32.494684Z","iopub.execute_input":"2024-02-26T05:23:32.495770Z","iopub.status.idle":"2024-02-26T05:23:36.721054Z","shell.execute_reply.started":"2024-02-26T05:23:32.495723Z","shell.execute_reply":"2024-02-26T05:23:36.719836Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"         user_id category        id  hour_mean  hour_max  hour_min  hour_std  \\\n0              0        0       0;0   8.500000         9         8  0.707107   \n1              3        0       3;0  13.833333        19        10  3.488075   \n2              4        0       4;0   9.428571        16         6  3.823486   \n3              5        0       5;0  12.642857        20         6  4.877060   \n4              7        0       7;0  11.800000        18         5  4.211096   \n...          ...      ...       ...        ...       ...       ...       ...   \n9838929    19991       99  19991;99   9.000000        12         6  4.242641   \n9838930    19992       99  19992;99  10.500000        12         9  2.121320   \n9838931    19993       99  19993;99   9.666667        11         7  2.309401   \n9838932    19994       99  19994;99  11.500000        16         8  3.696846   \n9838933    19997       99  19997;99  11.000000        11        11  0.000000   \n\n         week_mean  week_max  week_min  ...  rating_per_w_median  \\\n0        32.000000        35        29  ...                  0.0   \n1        28.666667        48        15  ...                  0.0   \n2        20.571429        28        16  ...                  0.0   \n3        29.142857        35        23  ...                  0.0   \n4        12.100000        26         1  ...                  0.0   \n...            ...       ...       ...  ...                  ...   \n9838929  35.000000        35        35  ...                  0.0   \n9838930  35.000000        35        35  ...                  0.0   \n9838931  35.666667        36        35  ...                  0.0   \n9838932  35.500000        36        35  ...                  0.0   \n9838933  35.500000        36        35  ...                  0.0   \n\n         rating_per_d_mean  rating_per_d_std  rating_per_d_sum  \\\n0                 0.966042          5.660171          8.250000   \n1                 0.152875          0.820443          1.305556   \n2                 0.241361          1.196201          2.061224   \n3                 0.066315          0.281990          0.566327   \n4                 0.100703          0.461075          0.860000   \n...                    ...               ...               ...   \n9838929           0.731850          4.387127          6.250000   \n9838930           1.083138          5.638909          9.250000   \n9838931           0.403331          2.278648          3.444444   \n9838932           0.219555          1.265042          1.875000   \n9838933           0.731850          4.216808          6.250000   \n\n         rating_per_d_median  rating_per_hour_mean  rating_per_hour_std  \\\n0                        0.0              0.966042             5.660171   \n1                        0.0              0.152875             0.820443   \n2                        0.0              0.241361             1.196201   \n3                        0.0              0.066315             0.281990   \n4                        0.0              0.100703             0.461075   \n...                      ...                   ...                  ...   \n9838929                  0.0              0.731850             4.387127   \n9838930                  0.0              1.083138             5.638909   \n9838931                  0.0              0.403331             2.278648   \n9838932                  0.0              0.219555             1.265042   \n9838933                  0.0              0.731850             4.216808   \n\n         rating_per_hour_sum  rating_per_hour_median  target  \n0                   8.250000                     0.0       0  \n1                   1.305556                     0.0       0  \n2                   2.061224                     0.0       0  \n3                   0.566327                     0.0       0  \n4                   0.860000                     0.0       1  \n...                      ...                     ...     ...  \n9838929             6.250000                     0.0       0  \n9838930             9.250000                     0.0       0  \n9838931             3.444444                     0.0       0  \n9838932             1.875000                     0.0       0  \n9838933             6.250000                     0.0       0  \n\n[9838934 rows x 60 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>category</th>\n      <th>id</th>\n      <th>hour_mean</th>\n      <th>hour_max</th>\n      <th>hour_min</th>\n      <th>hour_std</th>\n      <th>week_mean</th>\n      <th>week_max</th>\n      <th>week_min</th>\n      <th>...</th>\n      <th>rating_per_w_median</th>\n      <th>rating_per_d_mean</th>\n      <th>rating_per_d_std</th>\n      <th>rating_per_d_sum</th>\n      <th>rating_per_d_median</th>\n      <th>rating_per_hour_mean</th>\n      <th>rating_per_hour_std</th>\n      <th>rating_per_hour_sum</th>\n      <th>rating_per_hour_median</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0;0</td>\n      <td>8.500000</td>\n      <td>9</td>\n      <td>8</td>\n      <td>0.707107</td>\n      <td>32.000000</td>\n      <td>35</td>\n      <td>29</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.966042</td>\n      <td>5.660171</td>\n      <td>8.250000</td>\n      <td>0.0</td>\n      <td>0.966042</td>\n      <td>5.660171</td>\n      <td>8.250000</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>0</td>\n      <td>3;0</td>\n      <td>13.833333</td>\n      <td>19</td>\n      <td>10</td>\n      <td>3.488075</td>\n      <td>28.666667</td>\n      <td>48</td>\n      <td>15</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.152875</td>\n      <td>0.820443</td>\n      <td>1.305556</td>\n      <td>0.0</td>\n      <td>0.152875</td>\n      <td>0.820443</td>\n      <td>1.305556</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>0</td>\n      <td>4;0</td>\n      <td>9.428571</td>\n      <td>16</td>\n      <td>6</td>\n      <td>3.823486</td>\n      <td>20.571429</td>\n      <td>28</td>\n      <td>16</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.241361</td>\n      <td>1.196201</td>\n      <td>2.061224</td>\n      <td>0.0</td>\n      <td>0.241361</td>\n      <td>1.196201</td>\n      <td>2.061224</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>0</td>\n      <td>5;0</td>\n      <td>12.642857</td>\n      <td>20</td>\n      <td>6</td>\n      <td>4.877060</td>\n      <td>29.142857</td>\n      <td>35</td>\n      <td>23</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.066315</td>\n      <td>0.281990</td>\n      <td>0.566327</td>\n      <td>0.0</td>\n      <td>0.066315</td>\n      <td>0.281990</td>\n      <td>0.566327</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>0</td>\n      <td>7;0</td>\n      <td>11.800000</td>\n      <td>18</td>\n      <td>5</td>\n      <td>4.211096</td>\n      <td>12.100000</td>\n      <td>26</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.100703</td>\n      <td>0.461075</td>\n      <td>0.860000</td>\n      <td>0.0</td>\n      <td>0.100703</td>\n      <td>0.461075</td>\n      <td>0.860000</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9838929</th>\n      <td>19991</td>\n      <td>99</td>\n      <td>19991;99</td>\n      <td>9.000000</td>\n      <td>12</td>\n      <td>6</td>\n      <td>4.242641</td>\n      <td>35.000000</td>\n      <td>35</td>\n      <td>35</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.731850</td>\n      <td>4.387127</td>\n      <td>6.250000</td>\n      <td>0.0</td>\n      <td>0.731850</td>\n      <td>4.387127</td>\n      <td>6.250000</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9838930</th>\n      <td>19992</td>\n      <td>99</td>\n      <td>19992;99</td>\n      <td>10.500000</td>\n      <td>12</td>\n      <td>9</td>\n      <td>2.121320</td>\n      <td>35.000000</td>\n      <td>35</td>\n      <td>35</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.083138</td>\n      <td>5.638909</td>\n      <td>9.250000</td>\n      <td>0.0</td>\n      <td>1.083138</td>\n      <td>5.638909</td>\n      <td>9.250000</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9838931</th>\n      <td>19993</td>\n      <td>99</td>\n      <td>19993;99</td>\n      <td>9.666667</td>\n      <td>11</td>\n      <td>7</td>\n      <td>2.309401</td>\n      <td>35.666667</td>\n      <td>36</td>\n      <td>35</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.403331</td>\n      <td>2.278648</td>\n      <td>3.444444</td>\n      <td>0.0</td>\n      <td>0.403331</td>\n      <td>2.278648</td>\n      <td>3.444444</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9838932</th>\n      <td>19994</td>\n      <td>99</td>\n      <td>19994;99</td>\n      <td>11.500000</td>\n      <td>16</td>\n      <td>8</td>\n      <td>3.696846</td>\n      <td>35.500000</td>\n      <td>36</td>\n      <td>35</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.219555</td>\n      <td>1.265042</td>\n      <td>1.875000</td>\n      <td>0.0</td>\n      <td>0.219555</td>\n      <td>1.265042</td>\n      <td>1.875000</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9838933</th>\n      <td>19997</td>\n      <td>99</td>\n      <td>19997;99</td>\n      <td>11.000000</td>\n      <td>11</td>\n      <td>11</td>\n      <td>0.000000</td>\n      <td>35.500000</td>\n      <td>36</td>\n      <td>35</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.731850</td>\n      <td>4.216808</td>\n      <td>6.250000</td>\n      <td>0.0</td>\n      <td>0.731850</td>\n      <td>4.216808</td>\n      <td>6.250000</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>9838934 rows × 60 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"x_cols = Train.select_dtypes(include=['int', 'int8', 'int32', 'uint32', 'uint8', 'float']).drop(columns = ['user_id', 'target'], axis = 1).columns.tolist()\nprint(x_cols)\ny_cols = ['target']\nprint(y_cols)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T05:23:36.722607Z","iopub.execute_input":"2024-02-26T05:23:36.723604Z","iopub.status.idle":"2024-02-26T05:23:38.419477Z","shell.execute_reply.started":"2024-02-26T05:23:36.723562Z","shell.execute_reply":"2024-02-26T05:23:38.418203Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"['hour_mean', 'hour_max', 'hour_min', 'hour_std', 'week_mean', 'week_max', 'week_min', 'week_std', 'weekday_mean', 'weekday_max', 'weekday_min', 'weekday_std', 'day_mean', 'day_max', 'day_min', 'day_std', 'month_mean', 'month_max', 'month_min', 'month_std', 'year_mean', 'year_max', 'year_min', 'year_std', 'total_rating_mean', 'total_rating_max', 'total_rating_min', 'total_rating_std', 'ordered_mean', 'ordered_max', 'ordered_min', 'ordered_std', 'total_order_in_cat_mean', 'total_order_in_cat_max', 'total_order_in_cat_min', 'total_order_in_cat_std', 'total_order_num_mean', 'total_order_num_max', 'total_order_num_min', 'total_order_num_std', 'rating_per_m_mean', 'rating_per_m_std', 'rating_per_m_sum', 'rating_per_m_median', 'rating_per_w_mean', 'rating_per_w_std', 'rating_per_w_sum', 'rating_per_w_median', 'rating_per_d_mean', 'rating_per_d_std', 'rating_per_d_sum', 'rating_per_d_median', 'rating_per_hour_mean', 'rating_per_hour_std', 'rating_per_hour_sum', 'rating_per_hour_median']\n['target']\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datetime\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt, numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.cluster import AgglomerativeClustering\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import metrics\nimport warnings\nimport sys\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nnp.random.seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T05:23:38.420989Z","iopub.execute_input":"2024-02-26T05:23:38.421370Z","iopub.status.idle":"2024-02-26T05:23:38.793502Z","shell.execute_reply.started":"2024-02-26T05:23:38.421338Z","shell.execute_reply":"2024-02-26T05:23:38.792335Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nscaler = RobustScaler()\n\ns_Train = Train.copy()\ns_Train[x_cols] = scaler.fit_transform(s_Train[x_cols])\n\npca = PCA(n_components=21)\n\ns_Train_pca = pca.fit_transform(s_Train[x_cols])\n# test_X_scaled = pca.transform(test_X_scaled)\n\nTrain_pca = pd.DataFrame(s_Train_pca, columns=[f'pc{i+1}' for i in range(s_Train_pca.shape[1])])\nTrain_pca['target'] = Train['target'].values\nTrain_pca['id'] = Train['id'].values","metadata":{"execution":{"iopub.status.busy":"2024-02-26T05:23:38.797079Z","iopub.execute_input":"2024-02-26T05:23:38.797419Z","iopub.status.idle":"2024-02-26T05:26:36.866552Z","shell.execute_reply.started":"2024-02-26T05:23:38.797392Z","shell.execute_reply":"2024-02-26T05:26:36.864735Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Elbow Method to determine the number of clusters to be formed:')\nwarnings.filterwarnings(\"ignore\", message=\"findfont:.*\")\nElbow_M = KElbowVisualizer(KMeans(), k=21)\nElbow_M.fit(Train_pca.drop(columns = ['id', 'target'], axis = 1))\nElbow_M.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T05:26:36.868771Z","iopub.execute_input":"2024-02-26T05:26:36.869201Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Elbow Method to determine the number of clusters to be formed:\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=7)\nyhat_kmeans = kmeans.fit_predict(Train_pca.drop(columns=['id', 'target']))\n\n# Добавление информации о кластерах в исходный датафрейм\nTrain_pca[\"Clusters\"] = yhat_kmeans\ns_Train[\"Clusters\"] = yhat_kmeans\nTrain[\"Clusters\"] = yhat_kmeans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train.to_parquet('Train.parquet', index=False)\n# Train_pca.to_parquet('s_Train_pca.parquet', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_THREADS = 2 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTARGET_NAME = 'target' # Target column name\nTIMEOUT = 300","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Train_set, Valid_set = train_test_split(Train_pca, test_size = TEST_SIZE,\n                                        stratify = None, random_state = 23)\nTrain_set.reset_index(drop=True, inplace=True)\nValid_set.reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodel0 = BoostLGBM(\n    default_params={'learning_rate': 0.05, 'num_leaves': 64, 'seed': 42, 'num_threads': N_THREADS}\n)\npipe0 = LGBSimpleFeatures()\nmbie = ModelBasedImportanceEstimator()\nselector = ImportanceCutoffSelector(pipe0, model0, mbie, cutoff=0)\n\npipe = LGBSimpleFeatures()\n\nparams_tuner1 = OptunaTuner(n_trials=20, timeout=30) # stop after 20 iterations or after 30 seconds\nmodel1 = BoostLGBM(\n    default_params={'learning_rate': 0.05, 'num_leaves': 128, 'seed': 1, 'num_threads': N_THREADS}\n)\nmodel2 = BoostLGBM(\n    default_params={'learning_rate': 0.025, 'num_leaves': 64, 'seed': 2, 'num_threads': N_THREADS}\n)\n\npipeline_lvl1 = MLPipeline([\n    (model1, params_tuner1),\n    model2\n], pre_selection=selector, features_pipeline=pipe, post_selection=None)\n\npipe1 = LGBSimpleFeatures()\n\nmodel = BoostLGBM(\n    default_params={'learning_rate': 0.05, 'num_leaves': 64, 'max_bin': 1024, 'seed': 3, 'num_threads': N_THREADS},\n    freeze_defaults=True\n)\n\npipeline_lvl2 = MLPipeline([model], pre_selection=None, features_pipeline=pipe1, post_selection=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Подключение TPU\nimport tensorflow as tf\n\n# Создание кластера TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n# Соединение кластера\ntf.config.experimental_connect_to_cluster(tpu)\n\n# Инициализация TPU системы\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# Создание стратегии распределения на TPU\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# Здесь следует разместить весь код обучения модели, чтобы TensorFlow автоматически распределял вычисления на все ядра TPU\n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T05:05:38.583600Z","iopub.status.idle":"2024-02-26T05:05:38.584065Z","shell.execute_reply.started":"2024-02-26T05:05:38.583796Z","shell.execute_reply":"2024-02-26T05:05:38.583812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nTrain_set.reset_index(drop=True, inplace=True)\nValid_set.reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Train_set.iloc[:1000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \ndef f1 (real, pred, **kwargs):\n    return f1_score(real, (pred > 0.5).astype(int), **kwargs)\n\nROLES = {'target': TARGET_NAME, 'drop': ['id']}\nTASK = Task('binary', metric = f1)\nreader = PandasToPandasReader(TASK, cv=N_FOLDS, random_state=RANDOM_STATE)\n\n# default_lama_params = {\n#     \"task\": TASK,\n#     \"timeout\": TIMEOUT,\n#     \"cpu_limit\": N_THREADS,\n#     \"reader_params\": {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE}\n# }\n    \n# automl = TabularAutoML(\n#     **default_lama_params,\n#     general_params = {\"use_algos\": [[\"lgb\", \"mlp\", \"dense\"]]},\n#     nn_params = {\"n_epochs\": 10, \"bs\": 150, \"num_workers\": 2}\n   \n# )\n# automl = TabularAutoML(task = TASK, \n#                        timeout = 300,\n#                        cpu_limit = 4,\n#                        reader_params = {'n_jobs': 4, 'cv': 5, 'random_state': 23},\n#                        general_params = {'use_algos': [['linear_l2']]},\n#                       )\n\nautoml = AutoML(reader, [\n    [pipeline_lvl1],\n    [pipeline_lvl2],\n], skip_conn=False)\n\ntrain_pred = automl.fit_predict(Train_set, roles = ROLES, verbose = 3)\nprint('Score', \"%.5f\" % f1(Train_set.target, train_pred.data))\nvalid_pred = automl.predict(Valid_set)\nprint('Score on out of folds validation', \"%.5f\" % f1(Valid_set.target, valid_pred.data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_score = 0\nfor i in np.arange(0.01, 1.0, 0.01):\n    score = f1 = f1_score(Valid_set.target, (valid_pred.data > i).astype(int))\n    if score > best_score:\n        best_score = score\n        proba_split = i\n\nprint('At i =', \"%.2f\" % proba_split,'score is : ' \"%.5f\" % best_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Train","metadata":{"execution":{"iopub.status.busy":"2024-02-26T05:05:38.592388Z","iopub.status.idle":"2024-02-26T05:05:38.592755Z","shell.execute_reply.started":"2024-02-26T05:05:38.592570Z","shell.execute_reply":"2024-02-26T05:05:38.592584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Test = Train.copy()\nTest['order_number'] += 1 \nTest['ordered'] = Test['ordered'] + Test['target']\ntest_total_ordered = Test.groupby('category')['ordered'].sum()\nTest['total_ordered'] = Test['category'].map(test_total_ordered)\nTest['rating'] = Test['ordered'] / Test['order_number']\nTest = Test.drop('target', axis=1)\nTest.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T05:05:38.593781Z","iopub.status.idle":"2024-02-26T05:05:38.594168Z","shell.execute_reply.started":"2024-02-26T05:05:38.593972Z","shell.execute_reply":"2024-02-26T05:05:38.593987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = automl.predict(Test)\nprint('Train target mean:', \"%.5f\" % Train.target.mean())\nprint('Test target mean:', \"%.5f\" % (predictions.data > 0.5).astype(int).mean())","metadata":{"execution":{"iopub.status.busy":"2024-02-26T05:05:38.595366Z","iopub.status.idle":"2024-02-26T05:05:38.595807Z","shell.execute_reply.started":"2024-02-26T05:05:38.595616Z","shell.execute_reply":"2024-02-26T05:05:38.595633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"th = 0.5\ntrain_mean = Train.target.mean()\ntest_mean = (predictions.data > th).astype(int).mean()\n\nwhile test_mean < train_mean:\n    th -= 0.005\n    test_mean = (predictions.data > th).astype(int).mean()\n    \nprint('Threshold:', \"%.4f\" % th)\nprint('Train mean:', \"%.5f\" % train_mean)\nprint('New Test mean:', \"%.5f\" % test_mean)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T05:05:38.596667Z","iopub.status.idle":"2024-02-26T05:05:38.597332Z","shell.execute_reply.started":"2024-02-26T05:05:38.597135Z","shell.execute_reply":"2024-02-26T05:05:38.597153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Test['target'] = (predictions.data > th).astype(int)\nsubmit = pd.merge(sub['id'], Test[['id', 'target']], on='id')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T05:05:38.599162Z","iopub.status.idle":"2024-02-26T05:05:38.599701Z","shell.execute_reply.started":"2024-02-26T05:05:38.599421Z","shell.execute_reply":"2024-02-26T05:05:38.599444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\n\nwith open('submission.csv', 'w', newline='') as csvfile:\n    csvwriter = csv.writer(csvfile)\n    csvwriter.writerow(submit.columns)\n    for row in submit.values:\n        csvwriter.writerow(row)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T05:05:38.601387Z","iopub.status.idle":"2024-02-26T05:05:38.601948Z","shell.execute_reply.started":"2024-02-26T05:05:38.601660Z","shell.execute_reply":"2024-02-26T05:05:38.601683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit","metadata":{"execution":{"iopub.status.busy":"2024-02-26T05:05:38.603699Z","iopub.status.idle":"2024-02-26T05:05:38.604255Z","shell.execute_reply.started":"2024-02-26T05:05:38.603970Z","shell.execute_reply":"2024-02-26T05:05:38.603992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}